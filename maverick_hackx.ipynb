{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10Yash29/Plagiarism/blob/main/maverick_hackx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxJV0pdQEEA8",
        "outputId": "27eeb4c7-140a-4c16-e7dc-6889f463fec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWqcUvZkEIUS",
        "outputId": "04b744e1-5dbb-4cf1-9a7b-c0a131d7cb39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMg7pFKBETjS"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.set_verbosity_warning()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_auth_token=True)\n",
        "model = BertModel.from_pretrained('bert-base-uncased', use_auth_token=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPBxo9usEeyj",
        "outputId": "a33910ae-dfd5-435c-9731-2dbc5ccfa2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test submission is plagiarized.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    embeddings = last_hidden_states.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "DATABASE_DIR = '/content/training data/original_1'\n",
        "\n",
        "def load_training_data():\n",
        "    if not os.path.exists(DATABASE_DIR):\n",
        "        raise FileNotFoundError(f\"The directory {DATABASE_DIR} does not exist.\")\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    for filename in os.listdir(DATABASE_DIR):\n",
        "        file_path = os.path.join(DATABASE_DIR, filename)\n",
        "        with open(file_path, 'r') as f:\n",
        "            code = f.read().strip()\n",
        "\n",
        "            if len(code) == 0:\n",
        "                print(f\"Skipping empty file: {filename}\")\n",
        "                continue\n",
        "\n",
        "            data.append(code)\n",
        "\n",
        "            if 'original' in filename:\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "    if len(data) == 0:\n",
        "        raise ValueError(\"No valid code files found.\")\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "try:\n",
        "    data, labels = load_training_data()\n",
        "except (FileNotFoundError, ValueError) as e:\n",
        "    print(e)\n",
        "\n",
        "if data:\n",
        "    train_embeddings = [get_embedding(code) for code in data]\n",
        "\n",
        "    # Hardcoded test submission\n",
        "    test_code = \"\"\"\n",
        "    class Solution:\n",
        "        def searchMatrix(self, matrix: List[List[int]], target: int) -> bool:\n",
        "            top_row = 0\n",
        "            bottom_row = len(matrix) - 1\n",
        "            while top_row <= bottom_row:\n",
        "                middle_row = (top_row + bottom_row) // 2\n",
        "                if matrix[middle_row][0] < target and matrix[middle_row][-1] > target:\n",
        "                    break\n",
        "                elif matrix[middle_row][0] > target:\n",
        "                    bottom_row = middle_row - 1\n",
        "                else:\n",
        "                    top_row = middle_row + 1\n",
        "            row_to_search = (top_row + bottom_row) // 2\n",
        "            left_col = 0\n",
        "            right_col = len(matrix[row_to_search]) - 1\n",
        "            while left_col <= right_col:\n",
        "                middle_col = (left_col + right_col) // 2\n",
        "                if matrix[row_to_search][middle_col] == target:\n",
        "                    return True\n",
        "                elif matrix[row_to_search][middle_col] > target:\n",
        "                    right_col = middle_col - 1\n",
        "                else:\n",
        "                    left_col = middle_col + 1\n",
        "            return False\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    test_embedding = get_embedding(test_code)\n",
        "\n",
        "\n",
        "    similarities = [cosine_similarity(test_embedding.numpy(), train_embedding.numpy())[0][0] for train_embedding in train_embeddings]\n",
        "\n",
        "    if max(similarities) > 0.8:\n",
        "        print(\"The test submission is plagiarized.\")\n",
        "    else:\n",
        "        print(\"The test submission is original.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MedCBKYdJPyO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}